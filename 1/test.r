# На диаграмме видим что большая часть значений расположена в нижней части.
# Несколько значений очевидно выпадают из этой тенденции.

abline(h = 1000000, col = "red") (1.2)

# Исключим эти значения и построим диаграмму с учётом изменений. (2)
data <- data[-which(data$en > 1000000), c(1, 2)]
plot(data$ex, data$en)

# На диаграмме видим отсутствие линейной зависимости.
# Коэффициент корреляции будет низким.

# Находим значение коэффициента корреляции.
cor_result <- cor(data$ex, data$en)
print(cor_result)

# Коэффициент корреляции: 0.06402384
# Значение коэффицента < 0.5, что говорит о слабой корреляционной зависимости между этими переменными.

# Построим график регрессии (3.1)
plot(data$ex, data$en)
abline(lm_res, col = "red")

# Найдем математическое ожидание остатков
mean(lm_res$residuals)

# Математическое ожидание остатков приближается к нулю

#Построим график остатков (3.2)
plot(lm_res$residuals)
abline(h = 0)

# Остатки независимы и распределены неравномерно

# Найдем относительную ошибку аппроксимации MAPE
accuracy(lm_res)

#                         ME     RMSE    MAE       MPE     MAPE      MASE
# Training set -1.553949e-11 503046.9 192487 -11945.34 11962.05 0.9506542
# Относительная ошибка аппроксимации равна 11962.05%
# Регрессию нельзя использовать для прогнозирования

# Исследуем нелинейную регрессию с параметрами
nls_params_res <- nls(data$en ~ p1 + p2 * data$ex + p3 * (data$ex ^ 2), data = data, start = list(p1 = 0, p2 = 1000, p3 = -1000))
summary(nls_params_res)

# Исследуем нелинейную регрессию с параметрами
nls_params_res <- nls(data$en ~ p1 + p2 / (p3 + data$ex), data = data, start = list(p1 = 500, p2 = -10000, p3 = -1000))
summary(nls_params_res)

# Вычислим начальные параметры
range_ex <- range(data$ex)
range_en <- range(data$en)

initial_values <- list(p1 = mean(range_en), p2 = 1, p3 = 1)

# Проверим значимость коэффициентов уравнения:
# (Intercept) -4.856e+05 Std. Error 3.447e+05, t value -1.409, Pr(>|t|) 0.1624
# p-value для свободного члена (Intercept) = 0.1624 > 0.1, следовательно, свободный член не значим.
# data$ex 5.204e+01 Std. Error 3.058e+01, t value 1.702, Pr(>|t|) 0.0923
# p-value для коэффициента при переменной (data$ex) = 0.0923 < 0.1, следовательно, коэффициент при переменной значим.
# (data$ex^2) -7.745e-04 Std. Error 5.618e-04, t value -1.379, Pr(>|t|) 0.1715
# p-value для коэффициента при переменной (data$ex^2) = 0.1715 > 0.1, следовательно, коэффициент при переменной не значим.

# Вывод:
# Коэффициент при переменной data$ex значим, в то время как свободный член и коэффициент при переменной (data$ex^2) не значимы.

-----------------------------------------------------------------------------------------------------------

# Исследуем нелинейную регрессию с параметрами

nls_params_res <- nls(data$en ~ p1 + p2 * data$ex + p3 * data$ex^2, data = data, start = list(p1 = 500, p2 = 10000, p3 = -1000))
summary(nls_params_res)

# Проверим значимость коэффициентов уравнения:
# (Intercept) -4.856e+05 Std. Error 3.447e+05, t value -1.409, Pr(>|t|) 0.1624
# p-value для свободного члена (Intercept) = 0.1624 > 0.1, следовательно, свободный член не значим.
# data$ex 5.204e+01 Std. Error 3.058e+01, t value 1.702, Pr(>|t|) 0.0923
# p-value для коэффициента при переменной (data$ex) = 0.0923 < 0.1, следовательно, коэффициент при переменной значим.
# (data$ex^2) -7.745e-04 Std. Error 5.618e-04, t value -1.379, Pr(>|t|) 0.1715
# p-value для коэффициента при переменной (data$ex^2) = 0.1715 > 0.1, следовательно, коэффициент при переменной не значим.

# Вывод:
# Коэффициент при переменной data$ex значим, в то время как свободный член и коэффициент при переменной (data$ex^2) не значимы.

# Сохраним коэффициенты регрессии
nls_params_coef <- coef(nls_params_res)

# Строим диаграмму рассеяния (2)
plot(data$ex, data$en)
curve(nls_params_coef[1] + nls_params_coef[2] * x + nls_params_coef[3] * x^2, add = TRUE, col = "red")

# Строим график остатков
plot(nls_params_res$m$resid())
abline(h = 0, col = "red")

-----------------------------------------------------------------------------------------------------------

# Исследуем квадратичную регрессию
lm_square_res <- lm(data$en ~ data$ex + data$ex^2)
summary(lm_square_res)

-----------------------------------------------------------------------------------------------------------

# Исследуем полиномиальную регрессию en = p1 + p2 * log(ex) + ε

lm_poly_res <- lm(formula = data$en ~ poly(data$ex, 2))
summary(lm_poly_res)

# Проверка значимости регрессии:
# Multiple R-squared: 0.04689
# Коэффициент детерминации равен 0.04689 – низкое значение.
# p-value: 0.1209
# p-value = 0.1209 > 0.1, следовательно, регрессия не значима.

# Проверка значимости коэффициентов уравнения:
# (Intercept) 144930 Std. Error 53055, t value 2.732, Pr(>|t|) 0.00761
# p-value для свободного члена (Intercept) = 0.00761 < 0.1, следовательно, свободный член значим.
# poly(data$ex, 2)1 788697 Std. Error 506114, t value 1.558, Pr(>|t|) 0.12274
# p-value для первого коэффициента = 0.12274 > 0.1, следовательно, первый коэффициент не значим.
# poly(data$ex, 2)2 -697724 Std. Error 506114, t value -1.379, Pr(>|t|) 0.17152
# p-value для второго коэффициента = 0.17152 > 0.1, следовательно, второй коэффициент не значим.

# Вывод:
# Регрессия не является статистически значимой, так как p-value для регрессии и некоторых коэффициентов превышает уровень значимости 0.1.
